{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M8kZQUxGjsv6",
        "outputId": "27b1de44-2f3e-4f0f-bdf6-1beafc701c43"
      },
      "id": "M8kZQUxGjsv6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "beaca47e",
      "metadata": {
        "id": "beaca47e"
      },
      "outputs": [],
      "source": [
        "## Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "41368bc8",
      "metadata": {
        "id": "41368bc8"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g., pd.read_csv)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
        "from sklearn import svm\n",
        "from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFHKTrqXj9Ig",
        "outputId": "ff5c3312-8cb6-4baa-86fc-17bc1152ae38"
      },
      "id": "TFHKTrqXj9Ig",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Data/Data_Train.xlsx'\n",
        "file_path2 = '/content/drive/MyDrive/Data/Test_set.xlsx'"
      ],
      "metadata": {
        "id": "yPjc-ksPkIjp"
      },
      "id": "yPjc-ksPkIjp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading datasets\n",
        "\n",
        "train_df = pd.read_excel(file_path)\n",
        "test_df = pd.read_excel(file_path2)"
      ],
      "metadata": {
        "id": "cfQzR_A0kz7G"
      },
      "id": "cfQzR_A0kz7G",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a70d3cb",
      "metadata": {
        "id": "1a70d3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "de56a214-6113-40e3-ac82-038e088c2e43"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8d40229edf34>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Reading datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data_Train.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test_set.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_Train.xlsx'"
          ]
        }
      ],
      "source": [
        "## Reading datasets\n",
        "\n",
        "train_df = pd.read_excel('Data_Train.xlsx')\n",
        "test_df = pd.read_excel('Test_set.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "660d4350",
      "metadata": {
        "id": "660d4350"
      },
      "outputs": [],
      "source": [
        "## Append the dataset\n",
        "\n",
        "big_df = train_df._append(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ab8f60df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab8f60df",
        "outputId": "7eb85ec0-86df-47da-9a11-7fca69ffdcb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Airline             object\n",
              "Date_of_Journey     object\n",
              "Source              object\n",
              "Destination         object\n",
              "Route               object\n",
              "Dep_Time            object\n",
              "Arrival_Time        object\n",
              "Duration            object\n",
              "Total_Stops         object\n",
              "Additional_Info     object\n",
              "Price              float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Datatype details\n",
        "\n",
        "big_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "51314cc1",
      "metadata": {
        "id": "51314cc1"
      },
      "outputs": [],
      "source": [
        "## Creating a dataframe\n",
        "\n",
        "big_df['Date'] = big_df['Date_of_Journey'].str.split('/').str[0]\n",
        "big_df['Month'] = big_df['Date_of_Journey'].str.split('/').str[1]\n",
        "big_df['Year'] = big_df['Date_of_Journey'].str.split('/').str[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d0edce9",
      "metadata": {
        "id": "2d0edce9"
      },
      "outputs": [],
      "source": [
        "## Assign the type of features\n",
        "\n",
        "big_df['Date'] = big_df['Date'].astype(int)\n",
        "big_df['Month'] = big_df['Month'].astype(int)\n",
        "big_df['Year'] = big_df['Year'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "97c5b293",
      "metadata": {
        "id": "97c5b293"
      },
      "outputs": [],
      "source": [
        "## Droping unnessary colomn\n",
        "\n",
        "big_df=big_df.drop(['Date_of_Journey'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33e8e24d",
      "metadata": {
        "id": "33e8e24d"
      },
      "outputs": [],
      "source": [
        "## Cleaning Arrival Time features from unwanted infos\n",
        "\n",
        "big_df['Arrival_Time'] = big_df['Arrival_Time'] .str.split(' ').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7dc3866",
      "metadata": {
        "id": "a7dc3866"
      },
      "outputs": [],
      "source": [
        "## Replacce missing values\n",
        "\n",
        "big_df['Total_Stops']=big_df['Total_Stops'].fillna('1 stop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "93dfe489",
      "metadata": {
        "id": "93dfe489"
      },
      "outputs": [],
      "source": [
        "## Converting non-stop to 0 stop\n",
        "\n",
        "big_df['Total_Stops']=big_df['Total_Stops'].replace('non-stop','0 stop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f26b326c",
      "metadata": {
        "id": "f26b326c"
      },
      "outputs": [],
      "source": [
        "## Extract to stop number\n",
        "\n",
        "big_df['Stop'] = big_df['Total_Stops'].str.split(' ').str[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "52bcdbe3",
      "metadata": {
        "id": "52bcdbe3"
      },
      "outputs": [],
      "source": [
        "## Convert this value to int\n",
        "\n",
        "big_df['Stop'] = big_df['Stop'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b8d3143f",
      "metadata": {
        "id": "b8d3143f"
      },
      "outputs": [],
      "source": [
        "## Drop unnessary colomn\n",
        "\n",
        "big_df=big_df.drop(['Total_Stops'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "9f555663",
      "metadata": {
        "id": "9f555663"
      },
      "outputs": [],
      "source": [
        "## Prepare new colomns\n",
        "\n",
        "big_df['Arrival_Hour'] = big_df['Arrival_Time'] .str.split(':').str[0]\n",
        "big_df['Arrival_Minute'] = big_df['Arrival_Time'] .str.split(':').str[1]\n",
        "\n",
        "big_df['Arrival_Hour'] = big_df['Arrival_Hour'].astype(int)\n",
        "big_df['Arrival_Minute'] = big_df['Arrival_Minute'].astype(int)\n",
        "big_df=big_df.drop(['Arrival_Time'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e9226b95",
      "metadata": {
        "id": "e9226b95"
      },
      "outputs": [],
      "source": [
        "## Again\n",
        "\n",
        "big_df['Dep_Hour'] = big_df['Dep_Time'] .str.split(':').str[0]\n",
        "big_df['Dep_Minute'] = big_df['Dep_Time'] .str.split(':').str[1]\n",
        "big_df['Dep_Hour'] = big_df['Dep_Hour'].astype(int)\n",
        "big_df['Dep_Minute'] = big_df['Dep_Minute'].astype(int)\n",
        "big_df=big_df.drop(['Dep_Time'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "196e80c2",
      "metadata": {
        "id": "196e80c2"
      },
      "outputs": [],
      "source": [
        "## Extract route info\n",
        "\n",
        "big_df['Route_1'] = big_df['Route'] .str.split('→ ').str[0]\n",
        "big_df['Route_2'] = big_df['Route'] .str.split('→ ').str[1]\n",
        "big_df['Route_3'] = big_df['Route'] .str.split('→ ').str[2]\n",
        "big_df['Route_4'] = big_df['Route'] .str.split('→ ').str[3]\n",
        "big_df['Route_5'] = big_df['Route'] .str.split('→ ').str[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a8700529",
      "metadata": {
        "id": "a8700529"
      },
      "outputs": [],
      "source": [
        "## Replacing nans with mean\n",
        "\n",
        "big_df['Price'].fillna((big_df['Price'].mean()), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "51262131",
      "metadata": {
        "id": "51262131"
      },
      "outputs": [],
      "source": [
        "## Replace NaNs with None and improve data quality\n",
        "\n",
        "big_df['Route_1'].fillna(\"None\",inplace = True)\n",
        "big_df['Route_2'].fillna(\"None\",inplace = True)\n",
        "big_df['Route_3'].fillna(\"None\",inplace = True)\n",
        "big_df['Route_4'].fillna(\"None\",inplace = True)\n",
        "big_df['Route_5'].fillna(\"None\",inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2e904f73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2e904f73",
        "outputId": "4e286552-ec51-4586-e289-1232680ec52a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Price          Date         Month     Year          Stop  \\\n",
              "count  13354.000000  13354.000000  13354.000000  13354.0  13354.000000   \n",
              "mean    9087.064121     13.389846      4.710574   2019.0      0.826045   \n",
              "std     4124.447805      8.439060      1.165622      0.0      0.674608   \n",
              "min     1759.000000      1.000000      3.000000   2019.0      0.000000   \n",
              "25%     6135.250000      6.000000      3.000000   2019.0      0.000000   \n",
              "50%     9087.064121     12.000000      5.000000   2019.0      1.000000   \n",
              "75%    11087.000000     21.000000      6.000000   2019.0      1.000000   \n",
              "max    79512.000000     27.000000      6.000000   2019.0      4.000000   \n",
              "\n",
              "       Arrival_Hour  Arrival_Minute      Dep_Hour    Dep_Minute  \n",
              "count  13354.000000    13354.000000  13354.000000  13354.000000  \n",
              "mean      13.396061       24.664146     12.513254     24.507264  \n",
              "std        6.896145       16.559723      5.736273     18.832385  \n",
              "min        0.000000        0.000000      0.000000      0.000000  \n",
              "25%        8.000000       10.000000      8.000000      5.000000  \n",
              "50%       14.000000       25.000000     11.000000     25.000000  \n",
              "75%       19.000000       35.000000     18.000000     40.000000  \n",
              "max       23.000000       55.000000     23.000000     55.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecb98c37-d4cf-46e0-be77-58595617955f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Date</th>\n",
              "      <th>Month</th>\n",
              "      <th>Year</th>\n",
              "      <th>Stop</th>\n",
              "      <th>Arrival_Hour</th>\n",
              "      <th>Arrival_Minute</th>\n",
              "      <th>Dep_Hour</th>\n",
              "      <th>Dep_Minute</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.0</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "      <td>13354.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>9087.064121</td>\n",
              "      <td>13.389846</td>\n",
              "      <td>4.710574</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.826045</td>\n",
              "      <td>13.396061</td>\n",
              "      <td>24.664146</td>\n",
              "      <td>12.513254</td>\n",
              "      <td>24.507264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4124.447805</td>\n",
              "      <td>8.439060</td>\n",
              "      <td>1.165622</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.674608</td>\n",
              "      <td>6.896145</td>\n",
              "      <td>16.559723</td>\n",
              "      <td>5.736273</td>\n",
              "      <td>18.832385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1759.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6135.250000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9087.064121</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>25.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>11087.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>79512.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecb98c37-d4cf-46e0-be77-58595617955f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ecb98c37-d4cf-46e0-be77-58595617955f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ecb98c37-d4cf-46e0-be77-58595617955f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8d30225c-2472-49d2-9661-0d03819e313a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d30225c-2472-49d2-9661-0d03819e313a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8d30225c-2472-49d2-9661-0d03819e313a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "big_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "47b3162c",
      "metadata": {
        "id": "47b3162c"
      },
      "outputs": [],
      "source": [
        "## Remove the colomns\n",
        "\n",
        "big_df=big_df.drop(['Route'], axis=1)\n",
        "big_df=big_df.drop(['Duration'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7b358566",
      "metadata": {
        "id": "7b358566"
      },
      "outputs": [],
      "source": [
        "# Converting the Categorical into integer variable\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "lb_encode = LabelEncoder()\n",
        "big_df[\"Additional_Info\"] = lb_encode.fit_transform(big_df[\"Additional_Info\"])\n",
        "big_df[\"Airline\"] = lb_encode.fit_transform(big_df[\"Airline\"])\n",
        "big_df[\"Destination\"] = lb_encode.fit_transform(big_df[\"Destination\"])\n",
        "big_df[\"Source\"] = lb_encode.fit_transform(big_df[\"Source\"])\n",
        "big_df['Route_1']= lb_encode.fit_transform(big_df[\"Route_1\"])\n",
        "big_df['Route_2']= lb_encode.fit_transform(big_df[\"Route_2\"])\n",
        "big_df['Route_3']= lb_encode.fit_transform(big_df[\"Route_3\"])\n",
        "big_df['Route_4']= lb_encode.fit_transform(big_df[\"Route_4\"])\n",
        "big_df['Route_5']= lb_encode.fit_transform(big_df[\"Route_5\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9889af7c",
      "metadata": {
        "id": "9889af7c"
      },
      "outputs": [],
      "source": [
        "# # Missing value validation\n",
        "\n",
        "def missing_values_table(df):\n",
        "        # Total missing values\n",
        "        mis_val = df.isnull().sum()\n",
        "\n",
        "        # Percentage of missing values\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "\n",
        "        # Make a table with the results\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "        # Rename the columns\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "\n",
        "        # Sort the table by percentage of missing descending\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "\n",
        "        # Print some summary information\n",
        "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "\n",
        "        # Return the dataframe with missing information\n",
        "        return mis_val_table_ren_columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "76514a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "76514a00",
        "outputId": "8a8b2771-9cea-4349-d754-899f409aba26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your selected dataframe has 18 columns.\n",
            "There are 0 columns that have missing values.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Missing Values, % of Total Values]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a185fc8-ea4a-450e-982c-d9aa423a7090\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Values</th>\n",
              "      <th>% of Total Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a185fc8-ea4a-450e-982c-d9aa423a7090')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a185fc8-ea4a-450e-982c-d9aa423a7090 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a185fc8-ea4a-450e-982c-d9aa423a7090');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "missing_values_table(big_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d826e816",
      "metadata": {
        "id": "d826e816"
      },
      "outputs": [],
      "source": [
        "# Split it into test and train\n",
        "\n",
        "df_train = big_df[0:10683]\n",
        "df_test = big_df[10683:]\n",
        "df_test = df_test.drop(['Price'], axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6a6fde50",
      "metadata": {
        "id": "6a6fde50"
      },
      "outputs": [],
      "source": [
        "X = df_train.drop(['Price'], axis=1)\n",
        "y = df_train.Price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "baa7fcbe",
      "metadata": {
        "id": "baa7fcbe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a4bc7c63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "a4bc7c63",
        "outputId": "52d7b741-f342-4c63-af49-95451b0f28ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"http://i.imgur.com/QBuDOjs.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "## MODEL BUILDING\n",
        "\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url = \"http://i.imgur.com/QBuDOjs.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "04c01f06",
      "metadata": {
        "id": "04c01f06"
      },
      "outputs": [],
      "source": [
        "## LINEAR REGRESSION\n",
        "\n",
        "#Build our model method\n",
        "lm = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8ca6ba9a",
      "metadata": {
        "id": "8ca6ba9a"
      },
      "outputs": [],
      "source": [
        "#Build our cross validation method\n",
        "\n",
        "kfolds = KFold(n_splits=50,shuffle=True, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7c0ce16f",
      "metadata": {
        "id": "7c0ce16f"
      },
      "outputs": [],
      "source": [
        "def cv_rmse(model):\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
        "                                   scoring=\"neg_mean_squared_error\",\n",
        "                                   cv = kfolds))\n",
        "    return(rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "3a797f03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a797f03",
        "outputId": "d14dd724-4071-4cd2-886f-259cb325d565"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3238.316987636253"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "benchmark_model = make_pipeline(RobustScaler(),\n",
        "                                lm).fit(X=X_train, y=y_train)\n",
        "cv_rmse(benchmark_model).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7764b6ab",
      "metadata": {
        "id": "7764b6ab"
      },
      "outputs": [],
      "source": [
        "## Ridge Regression\n",
        "\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "def ridge_selector(k):\n",
        "    ridge_model = make_pipeline(RobustScaler(),\n",
        "                                RidgeCV(alphas = [k],\n",
        "                                        cv=kfolds)).fit(X_train, y_train)\n",
        "\n",
        "    ridge_rmse = cv_rmse(ridge_model).mean()\n",
        "    return(ridge_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3233a030",
      "metadata": {
        "id": "3233a030"
      },
      "outputs": [],
      "source": [
        "r_alphas = [.0001, .0003, .0005, .0007, .0009,\n",
        "          .01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 60, 70, 80]\n",
        "\n",
        "ridge_scores = []\n",
        "for alpha in r_alphas:\n",
        "    score = ridge_selector(alpha)\n",
        "    ridge_scores.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "19663856",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19663856",
        "outputId": "36e44167-d1ad-4ab6-bf21-690cfd99f3ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                RMSE\n",
              "0.0001   3238.316986\n",
              "0.0003   3238.316983\n",
              "0.0005   3238.316980\n",
              "0.0007   3238.316977\n",
              "0.0009   3238.316974\n",
              "0.0100   3238.316841\n",
              "0.0500   3238.316253\n",
              "0.1000   3238.315520\n",
              "0.3000   3238.312603\n",
              "1.0000   3238.302585\n",
              "3.0000   3238.275551\n",
              "5.0000   3238.250778\n",
              "10.0000  3238.198049\n",
              "15.0000  3238.157376\n",
              "20.0000  3238.127663\n",
              "30.0000  3238.097862\n",
              "50.0000  3238.141578\n",
              "60.0000  3238.209316\n",
              "70.0000  3238.305034\n",
              "80.0000  3238.427310"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45bb29f0-2ef6-4c32-b012-87f9cfd2ef9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0001</th>\n",
              "      <td>3238.316986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0003</th>\n",
              "      <td>3238.316983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0005</th>\n",
              "      <td>3238.316980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0007</th>\n",
              "      <td>3238.316977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0009</th>\n",
              "      <td>3238.316974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0100</th>\n",
              "      <td>3238.316841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.0500</th>\n",
              "      <td>3238.316253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.1000</th>\n",
              "      <td>3238.315520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.3000</th>\n",
              "      <td>3238.312603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0000</th>\n",
              "      <td>3238.302585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0000</th>\n",
              "      <td>3238.275551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0000</th>\n",
              "      <td>3238.250778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.0000</th>\n",
              "      <td>3238.198049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.0000</th>\n",
              "      <td>3238.157376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.0000</th>\n",
              "      <td>3238.127663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30.0000</th>\n",
              "      <td>3238.097862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50.0000</th>\n",
              "      <td>3238.141578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60.0000</th>\n",
              "      <td>3238.209316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70.0000</th>\n",
              "      <td>3238.305034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80.0000</th>\n",
              "      <td>3238.427310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45bb29f0-2ef6-4c32-b012-87f9cfd2ef9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45bb29f0-2ef6-4c32-b012-87f9cfd2ef9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45bb29f0-2ef6-4c32-b012-87f9cfd2ef9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fa451718-f7f0-453d-ab28-8f0dcd38cd6f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fa451718-f7f0-453d-ab28-8f0dcd38cd6f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fa451718-f7f0-453d-ab28-8f0dcd38cd6f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(r_alphas, ridge_scores, label='Ridge')\n",
        "plt.legend('center')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('score')\n",
        "\n",
        "ridge_score_table = pd.DataFrame(ridge_scores, r_alphas, columns=['RMSE'])\n",
        "ridge_score_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f601e48b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f601e48b",
        "outputId": "b550d238-6d88-40d5-bdc6-f877ab0a65a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3238.153926834791"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
        "\n",
        "ridge_model2 = make_pipeline(RobustScaler(),\n",
        "                            RidgeCV(alphas = alphas_alt,\n",
        "                                    cv=kfolds)).fit(X_train, y_train)\n",
        "\n",
        "cv_rmse(ridge_model2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f78f83c8",
      "metadata": {
        "id": "f78f83c8"
      },
      "outputs": [],
      "source": [
        "## Lasso Regression\n",
        "\n",
        "alphas = [0.00005, 0.0001, 0.0003, 0.0005, 0.0007,\n",
        "          0.0009, 0.01]\n",
        "alphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n",
        "           0.0006, 0.0007, 0.0008]\n",
        "\n",
        "\n",
        "lasso_model2 = make_pipeline(RobustScaler(),\n",
        "                             LassoCV(max_iter=10000000,\n",
        "                                    alphas = alphas2,\n",
        "                                    random_state = 42)).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f33e12f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "f33e12f0",
        "outputId": "f3d13668-ce03-4443-d907-6cec8e9f31d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "scores = lasso_model2.steps[1][1].mse_path_\n",
        "\n",
        "plt.plot(alphas2, scores, label='Lasso')\n",
        "plt.legend(loc='center')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('RMSE')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "26798d3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26798d3e",
        "outputId": "eb626b4c-1a1c-41e0-b346-5cd628af4a7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3238.3169506199984"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "cv_rmse(lasso_model2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1145ff05",
      "metadata": {
        "id": "1145ff05"
      },
      "outputs": [],
      "source": [
        "## Elastic Net Regression\n",
        "\n",
        "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
        "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
        "\n",
        "elastic_cv = make_pipeline(RobustScaler(),\n",
        "                           ElasticNetCV(max_iter=10000000, alphas=e_alphas,\n",
        "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
        "\n",
        "elastic_model3 = elastic_cv.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b2251578",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2251578",
        "outputId": "d0f60e36-8872-4396-bc07-edef8d9797eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3238.2960573475075"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "cv_rmse(elastic_model3).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "2ab54927",
      "metadata": {
        "id": "2ab54927",
        "outputId": "df5fa79d-af04-409e-a136-2c92c3adb46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "03602ad7",
      "metadata": {
        "id": "03602ad7"
      },
      "outputs": [],
      "source": [
        "## XGBoost\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7d6d6258",
      "metadata": {
        "id": "7d6d6258"
      },
      "outputs": [],
      "source": [
        "def modelfit(alg, dtrain, target, useTrainCV=True,\n",
        "             cv_folds=5, early_stopping_rounds=50):\n",
        "\n",
        "    if useTrainCV:\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(dtrain.values,\n",
        "                              label=y.values)\n",
        "\n",
        "        print(\"\\nGetting Cross-validation result..\")\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain,\n",
        "                          num_boost_round=alg.get_params()['n_estimators'],\n",
        "                          nfold=cv_folds,metrics='rmse',\n",
        "                          early_stopping_rounds=early_stopping_rounds,\n",
        "                          verbose_eval = True)\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "\n",
        "    #Fit the algorithm on the data\n",
        "    print(\"\\nFitting algorithm to data...\")\n",
        "    alg.fit(dtrain, target, eval_metric='rmse')\n",
        "\n",
        "    #Predict training set:\n",
        "    print(\"\\nPredicting from training data...\")\n",
        "    dtrain_predictions = alg.predict(dtrain)\n",
        "\n",
        "    #Print model report:\n",
        "    print(\"\\nModel Report\")\n",
        "    print(\"RMSE : %.4g\" % np.sqrt(mean_squared_error(target.values,\n",
        "                                             dtrain_predictions)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2a6a2f49",
      "metadata": {
        "id": "2a6a2f49"
      },
      "outputs": [],
      "source": [
        "xgb3 = XGBRegressor(learning_rate=0.1, n_estimators=200, max_depth=10,\n",
        "                    min_child_weight=5, gamma=0, subsample=0.7, max_bin=20,\n",
        "                    colsample_bytree=0.8, objective='reg:squarederror',  # Update here\n",
        "                    nthread=4, scale_pos_weight=1, seed=27, reg_alpha=0.00006)\n",
        "\n",
        "xgb_fit = xgb3.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "a46508b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a46508b3",
        "outputId": "d5817417-1c37-485c-8e03-5cb1d63b4b32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1297.6979798688478"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "cv_rmse(xgb_fit).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "7f6bf7db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "7f6bf7db",
        "outputId": "fd6fdd24-e161-4cb7-d994-ea53112b7106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=42, ...)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=None, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Assuming you have your features and target variable (X_train, y_train) defined\n",
        "# Replace 'reg:linear' with 'reg:squarederror'\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "66c9f1dc",
      "metadata": {
        "id": "66c9f1dc"
      },
      "outputs": [],
      "source": [
        "## Support vector machine\n",
        "\n",
        "from sklearn import svm\n",
        "svr_opt = svm.SVR(C = 100000, gamma = 1e-08)\n",
        "\n",
        "svr_fit = svr_opt.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "f202896e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f202896e",
        "outputId": "9109d161-8b7f-47bb-b52f-690c44303fee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4191.657759309572"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "cv_rmse(svr_fit).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "4e1225df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1225df",
        "outputId": "8fab5fc5-11ff-4be7-8366-12478f17a5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 205\n",
            "[LightGBM] [Info] Number of data points in the train set: 7478, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9139.209281\n"
          ]
        }
      ],
      "source": [
        "## Light GBM\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "lgbm_model = LGBMRegressor(\n",
        "    objective='regression',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=200,\n",
        "    max_bin=100,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=5,\n",
        "    feature_fraction=0.8,\n",
        "    feature_fraction_seed=9,\n",
        "    bagging_seed=9,\n",
        "    min_data_in_leaf=20,\n",
        "    min_sum_hessian_in_leaf=11\n",
        ")\n",
        "\n",
        "lgbm_fit = lgbm_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_rmse(lgbm_fit).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEG1WwTdtEDI",
        "outputId": "b365c87d-06e8-436c-de3d-fe0c2529bd29"
      },
      "id": "mEG1WwTdtEDI",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001233 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9090.826440\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002220 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9081.282071\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9102.406534\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002224 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9085.831120\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9095.445697\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002233 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 208\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9096.501958\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9089.868373\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001355 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9077.167065\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002078 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9095.611711\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002166 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9087.202312\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9081.061706\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9079.813831\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001209 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9093.802082\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001301 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9095.955010\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001298 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9079.566625\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001271 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9082.612188\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001303 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9079.234693\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9085.149680\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9086.614863\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001361 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9094.634922\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001208 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9086.847741\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002189 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9084.954915\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9094.097239\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002557 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9075.410641\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002161 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9105.827586\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002495 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9083.329162\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9077.656510\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002898 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9082.974019\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9084.095138\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9076.473875\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9087.484287\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002154 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9089.121215\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10469, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9095.087019\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9089.425215\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002134 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9086.600000\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9089.169341\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000955 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9074.336294\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9086.402292\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002245 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9086.983763\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9085.887011\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002142 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9072.853486\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002194 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9083.938968\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000966 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9094.782521\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002359 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9096.845177\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002315 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9095.991786\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002232 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9097.912225\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003026 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9087.796180\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9080.462655\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 208\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9079.415091\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001226 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 209\n",
            "[LightGBM] [Info] Number of data points in the train set: 10470, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9080.456829\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1411.4194777333657"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STACKING\n",
        "\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#setup models\n",
        "ridge = make_pipeline(RobustScaler(),\n",
        "                      RidgeCV(alphas = alphas_alt, cv=kfolds))\n",
        "\n",
        "lasso = make_pipeline(RobustScaler(),\n",
        "                      LassoCV(max_iter=10000000, alphas = alphas2,\n",
        "                              random_state = 42, cv=kfolds))\n",
        "\n",
        "elasticnet = make_pipeline(RobustScaler(),\n",
        "                           ElasticNetCV(max_iter=10000000, alphas=e_alphas,\n",
        "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
        "\n",
        "lightgbm = make_pipeline(RobustScaler(),\n",
        "                        LGBMRegressor(objective='regression',num_leaves=31,\n",
        "                              learning_rate=0.1, n_estimators=200,\n",
        "                              max_bin = 100, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.8,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11))\n",
        "\n",
        "\n",
        "xgboost = make_pipeline(RobustScaler(),\n",
        "                        XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
        "                     min_child_weight=5 ,gamma=0, subsample=0.7,\n",
        "                     colsample_bytree=0.8,objective= 'reg:linear',\n",
        "                     nthread=4,scale_pos_weight=1,seed=27, reg_alpha=0.00006))\n",
        "\n",
        "\n",
        "#stack\n",
        "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n",
        "                                            xgboost, lightgbm),\n",
        "                               meta_regressor=xgboost,\n",
        "                               use_features_in_secondary=True)\n",
        "\n",
        "\n",
        "\n",
        "#prepare dataframes\n",
        "stackX = np.array(X_train)\n",
        "stacky = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "ZaPBeW7IyGwN"
      },
      "id": "ZaPBeW7IyGwN",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_gen_model = stack_gen.fit(stackX, stacky)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81_3dgIcyV72",
        "outputId": "3ed0b6f1-d8ae-44cc-cc2d-6237b632abe5"
      },
      "id": "81_3dgIcyV72",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:18] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:19] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:20] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:21] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:22] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 203\n",
            "[LightGBM] [Info] Number of data points in the train set: 5982, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9207.465731\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001417 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 5982, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9120.777332\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001921 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 203\n",
            "[LightGBM] [Info] Number of data points in the train set: 5982, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9092.959880\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 201\n",
            "[LightGBM] [Info] Number of data points in the train set: 5983, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9168.609393\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 200\n",
            "[LightGBM] [Info] Number of data points in the train set: 5983, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9106.234665\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:25] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [17:05:32] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001794 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 205\n",
            "[LightGBM] [Info] Number of data points in the train set: 7478, number of used features: 16\n",
            "[LightGBM] [Info] Start training from score 9139.209281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "em_preds = elastic_model3.predict(X_test)\n",
        "lasso_preds = lasso_model2.predict(X_test)\n",
        "ridge_preds = ridge_model2.predict(X_test)\n",
        "stack_gen_preds = stack_gen_model.predict(X_test)\n",
        "xgb_preds = xgb_fit.predict(X_test)\n",
        "#svr_preds = svr_fit.predict(X_test)\n",
        "lgbm_preds = lgbm_fit.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iHD31AyyklK",
        "outputId": "c5a3b077-df1d-44a5-a24d-d059fa6aea2f"
      },
      "id": "1iHD31AyyklK",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, stack_gen_preds))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcDlITOOynZe",
        "outputId": "5bf06efb-f346-4372-977b-872da375b90b"
      },
      "id": "ZcDlITOOynZe",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1448.526691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_xgb = df_test[['Additional_Info', 'Airline', 'Destination', 'Source', 'Date', 'Month',\n",
        "       'Year', 'Stop', 'Arrival_Hour', 'Arrival_Minute', 'Dep_Hour',\n",
        "       'Dep_Minute', 'Route_1', 'Route_2', 'Route_3', 'Route_4', 'Route_5']]\n",
        "preds_1 = stack_gen_model.predict(df_test_xgb)\n",
        "df_test_xgb['Price'] = preds_1\n",
        "df_test_xgb.to_csv('/content/drive/MyDrive/Data/flight_price_5.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oa9zHIyysch",
        "outputId": "fd02fa24-e796-48e2-85ef-a736fa1e7b68"
      },
      "id": "7oa9zHIyysch",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but RobustScaler was fitted without feature names\n",
            "  warnings.warn(\n",
            "<ipython-input-60-4a4d31ca200b>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test_xgb['Price'] = preds_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the feature names used during training\n",
        "trained_model_feature_names = xgb_fit.get_booster().feature_names\n",
        "print(\"Trained Model Feature Names:\", trained_model_feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SduzH9Ec2EQE",
        "outputId": "801f8bf5-5619-4676-dd89-62e794a9b04e"
      },
      "id": "SduzH9Ec2EQE",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Model Feature Names: ['Airline', 'Source', 'Destination', 'Additional_Info', 'Date', 'Month', 'Year', 'Stop', 'Arrival_Hour', 'Arrival_Minute', 'Dep_Hour', 'Dep_Minute', 'Route_1', 'Route_2', 'Route_3', 'Route_4', 'Route_5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the columns in your DataFrame\n",
        "df_columns = df_test_xgb.columns\n",
        "print(\"DataFrame Columns:\", df_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-1tKyYP2pub",
        "outputId": "180b5271-9dc0-4139-9475-a1f59d001017"
      },
      "id": "q-1tKyYP2pub",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Columns: Index(['Additional_Info', 'Airline', 'Destination', 'Source', 'Date', 'Month',\n",
            "       'Year', 'Stop', 'Arrival_Hour', 'Arrival_Minute', 'Dep_Hour',\n",
            "       'Dep_Minute', 'Route_1', 'Route_2', 'Route_3', 'Route_4', 'Route_5'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reorder or rename columns in your DataFrame\n",
        "df_test_xgb = df_test_xgb[trained_model_feature_names]\n"
      ],
      "metadata": {
        "id": "oEy0zlBx3D4L"
      },
      "id": "oEy0zlBx3D4L",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the corrected DataFrame\n",
        "preds_1 = xgb_fit.predict(df_test_xgb)\n",
        "\n",
        "# Add 'Price' column to the DataFrame\n",
        "df_test_xgb['Price'] = preds_1\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_path = '/content/drive/MyDrive/Data/flight_price_10.csv'\n",
        "df_test_xgb.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6r01wV63JxT",
        "outputId": "5e0e3474-a100-41c3-c993-946e690dfe3c"
      },
      "id": "t6r01wV63JxT",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-1154de1379be>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test_xgb['Price'] = preds_1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}